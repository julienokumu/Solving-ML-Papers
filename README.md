Mistral7B Paper ✅ 	

Attention Is All You Need ✅

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ✅

AlexNet ✅

ViT: An Image is 16x16 words ✅
