Mistral7B ✅ 	

Attention Is All You Need ✅

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ✅

AlexNet ✅

ViT: An Image is 16x16 words ✅

Mixtral Of Experts ✅

LLaMA2 ✅

LLaMA ✅
